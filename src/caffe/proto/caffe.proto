// Copyright 2013 Yangqing Jia

package caffe;

message BlobProto {
  optional int32 num = 1 [default = 0];
  optional int32 channels = 2 [default = 0];
  optional int32 height = 3 [default = 0];
  optional int32 width = 4 [default = 0];
  repeated float data = 5 [packed=true];
  repeated float diff = 6 [packed=true];
}

message Datum {
  optional int32 channels = 1;
  optional int32 height = 2;
  optional int32 width = 3;
  // the actual image data, in bytes
  optional bytes data = 4;
  optional int32 label = 5;
  // Optionally, the datum could also hold float data.
  repeated float float_data = 6;
}

message FillerParameter {
  // The filler type.
  optional string type = 1 [default = 'constant'];
  optional float value = 2 [default = 0]; // the value in constant filler
  optional float min = 3 [default = 0]; // the min value in uniform filler
  optional float max = 4 [default = 1]; // the max value in uniform filler
  optional float mean = 5 [default = 0]; // the mean value in gaussian filler
  optional float std = 6 [default = 1]; // the std value in gaussian filler
}

message LayerParameter {
  optional string name = 1; // the layer name
  optional string type = 2; // the string to specify the layer type

  // Parameters to specify layers with inner products.
  optional uint32 num_output = 3; // The number of outputs for the layer
  optional bool biasterm = 4 [default = true]; // whether to have bias terms
  optional FillerParameter weight_filler = 5; // The filler for the weight
  optional FillerParameter bias_filler = 6; // The filler for the bias

  optional uint32 pad = 7 [default = 0]; // The padding size
  optional uint32 kernelsize = 8; // The kernel size
  optional uint32 group = 9 [default = 1]; // The group size for group conv
  optional uint32 stride = 10 [default = 1]; // The stride
  enum PoolMethod {
    MAX = 0;
    AVE = 1;
  }
  optional PoolMethod pool = 11 [default = MAX]; // The pooling method
  optional float dropout_ratio = 12 [default = 0.5]; // dropout ratio

  optional uint32 local_size = 13 [default = 5]; // for local response norm
  optional float alpha = 14 [default = 1.]; // for local response norm
  optional float beta = 15 [default = 0.75]; // for local response norm

  // For data layers, specify the data source
  optional string source = 16;
  // For data pre-processing, we can do simple scaling and subtracting the
  // data mean, if provided. Note that the mean subtraction is always carried
  // out before scaling.
  optional float scale = 17 [ default = 1 ];
  optional string meanfile = 18;
  // For data layers, specify the batch size.
  optional uint32 batchsize = 19;
  // For data layers, specify if we would like to randomly crop an image.
  optional uint32 cropsize = 20 [default = 0];
  // For data layers, specify if we want to randomly mirror data.
  optional bool mirror = 21 [default = false];

  // The blobs containing the numeric parameters of the layer
  repeated BlobProto blobs = 50;
  // The ratio that is multiplied on the global learning rate. If you want to set
  // the learning ratio for one blob, you need to set it for all blobs.
  repeated float blobs_lr = 51;
}

message LayerConnection {
  optional LayerParameter layer = 1; // the layer parameter
  repeated string bottom = 2; // the name of the bottom blobs
  repeated string top = 3; // the name of the top blobs
}

message NetParameter {
  optional string name = 1; // consider giving the network a name
  repeated LayerConnection layers = 2; // a bunch of layers.
  repeated string input = 3; // The input to the network
}

message SolverParameter {
  optional float base_lr = 1; // The base learning rate
  // the number of iterations between displaying info. If display = 0, no info
  // will be displayed.
  optional int32 display = 2;
  optional int32 max_iter = 3; // the maximum number of iterations
  optional int32 snapshot = 4 [default = 0]; // The snapshot interval
  optional string lr_policy = 5; // The learning rate decay policy.
  optional float min_lr = 6 [default = 0]; // The mininum learning rate
  optional float max_lr = 7 [default = 1e10]; // The maximum learning rate
  optional float gamma = 8; // The parameter to compute the learning rate.
  optional float power = 9; // The parameter to compute the learning rate.
  optional float momentum = 10; // The momentum value.
  optional float weight_decay = 11; // The weight decay.
  optional float stepsize = 12; // the stepsize for learning rate policy "step"

  optional string snapshot_prefix = 13; // The prefix for the snapshot.
  // whether to snapshot diff in the results or not. Snapshotting diff will help
  // debugging but the final protocol buffer size will be much larger.
  optional bool snapshot_diff = 14 [ default = false];
  // Adagrad solver parameters
  // For Adagrad, we will first run normal sgd using the sgd parameters above
  // for adagrad_skip iterations, and then kick in the adagrad algorithm, with
  // the learning rate being adagrad_gamma * adagrad_skip. Note that the adagrad
  // algorithm will NOT use the learning rate multiplier that is specified in
  // the layer parameter specifications, as it will adjust the learning rate
  // of individual parameters in a data-dependent way.
  //    WORK IN PROGRESS: not actually implemented yet.
  optional float adagrad_gamma = 15; // adagrad learning rate multiplier
  optional float adagrad_skip = 16; // the steps to skip before adagrad kicks in
}

// A message that stores the solver snapshots
message SolverState {
  optional int32 iter = 1; // The current iteration
  optional string learned_net = 2; // The file that stores the learned net.
  repeated BlobProto history = 3; // The history for sgd solvers
}